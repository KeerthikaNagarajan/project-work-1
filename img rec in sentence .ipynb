{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650bb25-6e3b-4a32-8dbe-a953d04000a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted successfully.\n",
      "Transcription:  This is a Formula 1 car. This is especially a Mercedes-Benz petrolniss F1 car. This is Louis Hamilton's car. Its number is 34. He is a 7-time Volvo champion and he is awesome.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription Summary: This is a Formula 1 car. This is especially a Mercedes-Benz petrolniss F1 car. Louis Hamilton is a 7-time Volvo champion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nkeer/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-12 Python-3.12.4 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene Description: In this video, you see motorcycle and car. The narration says:  This is a Formula 1 car. This is especially a Mercedes-Benz petrolniss F1 car. This is Louis Hamilton's car. Its number is 34. He is a 7-time Volvo champion and he is awesome.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import (DistilBertTokenizer, DistilBertForQuestionAnswering,\n",
    "                          MarianMTModel, MarianTokenizer, pipeline)\n",
    "import whisper\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import cv2\n",
    "\n",
    "# Function to extract audio from a video file\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    \"\"\"Extract audio from the specified video file and save it as a .wav file.\"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(output_audio_path, codec='pcm_s16le')  # Use a lossless codec\n",
    "\n",
    "# Function to transcribe audio using Whisper\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio to text using Whisper ASR.\"\"\"\n",
    "    model = whisper.load_model(\"base\")  # Use the 'base' model for a balance of speed and accuracy\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result['text']\n",
    "\n",
    "# Function to summarize the transcription using a summarization pipeline\n",
    "def summarize_transcription(transcription):\n",
    "    \"\"\"Summarize the transcription using a BART summarization model.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    # Split the transcription into chunks if it's too long\n",
    "    max_length = 1024  # BART's maximum token limit\n",
    "    inputs = [transcription[i:i + max_length] for i in range(0, len(transcription), max_length)]\n",
    "    \n",
    "    summary = []\n",
    "    for input_text in inputs:\n",
    "        summary_chunk = summarizer(input_text, max_length=50, min_length=20, do_sample=False)\n",
    "        summary.append(summary_chunk[0]['summary_text'])\n",
    "    \n",
    "    return \" \".join(summary)\n",
    "\n",
    "# Function to initialize the DistilBERT model and tokenizer for question answering\n",
    "def init_qa_model():\n",
    "    \"\"\"Initialize the DistilBERT model and tokenizer for question answering.\"\"\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to answer questions based on context using DistilBERT\n",
    "def answer_question(question, context, tokenizer, model):\n",
    "    \"\"\"Answer a question based on the provided context.\"\"\"\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits) + 1\n",
    "    answer_tokens = input_ids[0][start_index:end_index]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    return answer.strip()  # Return trimmed answer\n",
    "\n",
    "# Function to load translation model and tokenizer\n",
    "def load_translation_model(source_lang, target_lang):\n",
    "    \"\"\"Load translation model and tokenizer.\"\"\"\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to translate text using MarianMT\n",
    "def translate_text(text, model, tokenizer):\n",
    "    \"\"\"Translate text using the MarianMT model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text.strip()  # Return trimmed translation\n",
    "\n",
    "# Function for text-to-speech\n",
    "def text_to_speech(text, language_code='en'):\n",
    "    \"\"\"Convert text to speech and play it.\"\"\"\n",
    "    if text:\n",
    "        audio_file = f\"answer_{int(time.time())}.mp3\"\n",
    "        tts = gTTS(text, lang=language_code)\n",
    "        tts.save(audio_file)\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(audio_file)\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            pygame.time.Clock().tick(10)\n",
    "        pygame.mixer.music.stop()\n",
    "        pygame.mixer.quit()\n",
    "        os.remove(audio_file)  # Clean up the audio file\n",
    "\n",
    "# Function to load the YOLOv5 model\n",
    "def load_yolov5_model():\n",
    "    \"\"\"Load YOLOv5 object detection model.\"\"\"\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Use the small model for speed\n",
    "    return model\n",
    "\n",
    "# Function to perform object detection on video frames and collect detected objects\n",
    "def detect_objects_in_video(video_path, model):\n",
    "    \"\"\"Detect objects in the video and return a list of detected objects per frame.\"\"\"\n",
    "    detected_objects = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_skip = 5  # Process every 5th frame to improve speed\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform inference only on every nth frame\n",
    "        if cap.get(cv2.CAP_PROP_POS_FRAMES) % frame_skip == 0:\n",
    "            results = model(frame)\n",
    "            frame_objects = [results.names[int(cls)] for *box, conf, cls in results.xyxy[0]]  # List of detected object names\n",
    "            detected_objects.append(frame_objects)\n",
    "\n",
    "            # Optionally render results on the frame\n",
    "            frame = results.render()[0]\n",
    "            cv2.imshow('YOLOv5 Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return detected_objects\n",
    "\n",
    "# Function to create a meaningful scene description\n",
    "def create_scene_description(detected_objects, transcription):\n",
    "    \"\"\"Create a description of the scene based on detected objects and transcription.\"\"\"\n",
    "    object_count = {}\n",
    "    \n",
    "    for objects in detected_objects:\n",
    "        for obj in objects:\n",
    "            object_count[obj] = object_count.get(obj, 0) + 1\n",
    "\n",
    "    most_common_objects = [obj for obj, count in object_count.items() if count > 1]\n",
    "    scene_summary = []\n",
    "\n",
    "    if most_common_objects:\n",
    "        scene_summary.append(f\"In this video, you see {' and '.join(most_common_objects)}.\")\n",
    "\n",
    "    # Combine with transcription for context\n",
    "    full_description = \" \".join(scene_summary)\n",
    "    full_description += f\" The narration says: {transcription}\"\n",
    "    return full_description.strip()  # Return trimmed description\n",
    "\n",
    "# Main function to run the entire process\n",
    "def main(video_path):\n",
    "    \"\"\"Main function to execute video processing and user interaction.\"\"\"\n",
    "    # Step 1: Extract audio from the video\n",
    "    audio_path = \"extracted_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "    print(\"Audio extracted successfully.\")\n",
    "\n",
    "    # Step 2: Transcribe audio to text\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "    # Step 3: Summarize transcription\n",
    "    transcription_summary = summarize_transcription(transcription)\n",
    "    print(\"Transcription Summary:\", transcription_summary)\n",
    "\n",
    "    # Step 4: Load the YOLOv5 model and detect objects in the video\n",
    "    yolov5_model = load_yolov5_model()\n",
    "    detected_objects = detect_objects_in_video(video_path, yolov5_model)\n",
    "\n",
    "    # Step 5: Create scene descriptions\n",
    "    scene_description = create_scene_description(detected_objects, transcription)\n",
    "    print(\"Scene Description:\", scene_description)\n",
    "\n",
    "    # Convert the scene description to speech\n",
    "    text_to_speech(scene_description)\n",
    "\n",
    "    # Initialize the question-answering model\n",
    "    tokenizer, model = init_qa_model()\n",
    "\n",
    "    # Step 6: Loop for answering questions\n",
    "    while True:\n",
    "        question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = answer_question(question, transcription, tokenizer, model)\n",
    "        print(\"Answer:\", answer)\n",
    "\n",
    "        # Step 7: Translate answer if desired\n",
    "        translate = input(\"Do you want to translate the answer? (yes/no): \").lower()\n",
    "        if translate == 'yes':\n",
    "            target_lang = input(\"Enter target language code (e.g., 'fr' for French, 'es' for Spanish): \").lower()\n",
    "            translation_model, translation_tokenizer = load_translation_model('en', target_lang)\n",
    "            translated_answer = translate_text(answer, translation_model, translation_tokenizer)\n",
    "            print(f\"Translated Answer ({target_lang}):\", translated_answer)\n",
    "            text_to_speech(translated_answer, target_lang)\n",
    "        else:\n",
    "            text_to_speech(answer)\n",
    "\n",
    "# Run the main function with your video file path\n",
    "if __name__ == \"__main__\":\n",
    "    video_file_path = \"44.mp4\"  # Replace with your video file path\n",
    "    main(video_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84dcfb-a160-4e77-a905-9a4903dd6f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
